---
title: "P8160 Group Project Presentation"
subtitle: "Optimization and Bootstrap"
author: "Xinlei Chen, Guojing Wu, Yujing Yao"
institute: "Department of Biostatistics, Columbia University"
date: "April 19, 2019"
header-includes:
- \usepackage{booktabs}
- \usepackage{makecell}
- \usepackage{indentfirst}
- \usepackage{graphicx}
- \usepackage{geometry}
- \usepackage{subfigure}
- \usepackage{amsmath}
- \usepackage{listings}
- \usepackage{tikz}
- \usetikzlibrary{matrix}
output:
  beamer_presentation:
    slide_level: 3
    toc: true
    theme: "Warsaw"
    colortheme: "whale"
    fonttheme: "structurebold"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, comment = "")
library(tidyverse)
library(Hmisc)#for errvar
library(ggcorrplot) # for correlation heatmap
library(ggpubr) # common legend
library(matrixcalc) #is.negative.def
library(kableExtra) # table
library(glmnet) # lasso
library(grid)
library(gridExtra)
# parallel program
library(parallel)
library(doParallel)
library(foreach)
library(iterators)
nCores <- 3
registerDoParallel(nCores) 
set.seed(99999)
options(knitr.table.format = "latex")
theme_set(theme_bw())
```

<!-- # level1 -->
<!-- ## level2 -->
<!-- ### level3 -->
# Introduction 
## Introduction 
### Introduction 

**Group project 2:** Optimization algorithms on a breast cancer diagnosis dataset

- **Aim: ** Build a predictive model based on logistic regression to faciliate cancer diagnosis, and we compared methods including Newton Raphson, Gradient Decent with general logistic regression and Pathwise Coordinate Descent with regularized logistic regression

### Introduction 

**Group project 3:** Bootstrapping on developing classification model

- **Aim: ** Build a predictive model based on logistic regression to faciliate down syndrome diagnosis, and compared methods including and Pathwise Coordinate Descent with regularized logistic regression and smoothed bootstrap estimation.

# Project: Optimization
## Background
### Breast Cancer Data

The data breast-cancer.csv have 569 row and 33 columns. The first two columns inclues covariate “ID” which lables individual breast tissue images and covariate “Diagnonsis” which indentifies if the image is coming from cancer tissue or benign cases. There are 357 benign and 212 malignant cases. The other 30 columns correspond to mean, standard deviation and the largest values (points on the tails) of the distributions of 10 features (radiusm texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension) computed for the cellnuclei.


### Multicollinearity Plot of the Dataset

- **Variable Selection:** Reduce multicollinearity based on both correlation coefficient and eigenvalue of correlation matrix

```{r, echo=FALSE, fig.height=4}
###### data manipulation
mydata <- read.csv("breast-cancer-1.csv") 
n <- dim(mydata)[1]
p <- dim(mydata)[2]
list <- c(3:(p - 1))
namesX <- names(mydata)[-c(1,2,p)]
# standardize
dataX <- do.call(cbind, lapply(list, function(x) (mydata[,x] - mean(mydata[,x]))/sd(mydata[,x]))) 
# design matrix
X <- data.frame(dataX) %>% 
  mutate(., intercept = 1)
# response
resp <- as.vector(ifelse(mydata[,2] == "M", 1, 0))
###### plot to check collinearity
colnames(dataX) <- namesX
colinearity.plot <- function(data){
  data.frame(data) %>% 
  select(starts_with("radius"), 
         starts_with("texture"), 
         starts_with("perimeter"), 
         starts_with("area"), 
         starts_with("smooth"), 
         starts_with("compact"), 
         starts_with("concavity"), 
         starts_with("concave"), 
         starts_with("symmetry"), 
         starts_with("fractal")) %>% 
  cor() %>% 
  ggcorrplot(.,ggtheme = ggplot2::theme_gray,
             colors = c("#6D9EC1", "white", "#E46726"),
             tl.cex = 6)}
g1 <- colinearity.plot(dataX)
newdataX <- read.csv("newdataX.csv",header = T)%>%
  dplyr::select(.,-c(1))
g2 <- colinearity.plot(newdataX)
ggarrange(g1,g2, ncol=2, nrow=1, common.legend = TRUE, legend = "bottom", labels = "AUTO")
```

## Method
### Logistic Model

```{r, echo=FALSE}
###### 0.logistic regression
# logfit.0 <- glm(resp~dataX, family = binomial(link = "logit"))
# algorithm didn't converge without delete colinearity
logdata <- cbind.data.frame(resp,newdataX)
logfit.1 <- glm(resp~., family = binomial(link = "logit"),data = logdata)
logit.beta <- coef(logfit.1)
##### 1. classical newton raphson
newX <- data.frame(newdataX) %>%
  mutate(., intercept = 1)
newdat <- list(y = resp, X = as.matrix(newX))
# function: calcualte loglik, gradient, hessian
logisticstuff <- function(dat, betavec){ 
  u <- dat$X %*% betavec
  expu <- exp(u) 
  loglik <- t(u) %*% dat$y - sum((log(1 + expu))) 
  prob <- expu / (1 + expu) 
  grad <- t(dat$X) %*% (dat$y - prob)
  Hess <- -t(dat$X) %*% diag(as.vector(prob*(1 - prob))) %*% dat$X 
  return(list(loglik = loglik, grad = grad, Hess = Hess))}

NewtonRaphson <- function(dat, func, start, tol=1e-10, maxiter =200){ 
  i <- 0 
  cur <- start 
  stuff <- func(dat, cur) 
  res <- c(0, stuff$loglik, cur)
  prevloglik <- -Inf 
  while (i < maxiter && abs(stuff$loglik - prevloglik) > tol && stuff$loglik > -Inf) {
    i <- i + 1 
    prevloglik <- stuff$loglik 
    prev <- cur 
    lambda = 0
    cur <- prev - ((1/(2^lambda)) * solve(stuff$Hess)) %*% stuff$grad
    while (func(dat, cur)$loglik < prevloglik){ # step-halving
      lambda = lambda + 1
      cur <- prev - ((1/(2^lambda)) * solve(stuff$Hess)) %*% stuff$grad
    }
    stuff <- func(dat, cur) 
    res <- rbind(res, c(i, stuff$loglik, cur))}
  
  return(res)
}
newres <- NewtonRaphson(newdat,logisticstuff, start = rep(0, dim(newdat$X)[2]))
newton.beta <- newres[nrow(newres),3:dim(newres)[2]]
##### 2. gradient descent
gradient <- function(dat, func, start, tol=1e-10, maxiter = 200){ 
  i <- 0 
  cur <- start 
  pp <- length(start)
  stuff <- func(dat, cur) 
  hessinversed <- solve(t(dat$X) %*% (dat$X))
  res <- c(0, stuff$loglik, cur)
  prevloglik <- -Inf
  while (i < maxiter && abs(stuff$loglik - prevloglik) > tol && stuff$loglik > -Inf) { 
    i <- i + 1 
    prevloglik <- stuff$loglik 
    prev <- cur 
    cur <- prev + hessinversed %*% (stuff$grad)
    stuff <- func(dat, cur) 
    res <- rbind(res, c(i, stuff$loglik, cur))} 
  return(res)
}
# gradres <- gradient(newdat, logisticstuff, start = rep(0, dim(newdat$X)[2]), maxiter = 1000)
gradres <- read.csv("gradres.csv",header = T)%>%
  dplyr::select(.,-c(1))
grad.beta <- gradres[nrow(gradres),3:dim(gradres)[2]]
##### 3. coordinate-wise logistic lasso
sfun <- function(beta,lambda) sign(beta) * max(abs(beta)-lambda, 0)
coordinatelasso <- function(lambda, dat, s, tol=1e-10, maxiter = 200){
  i <- 0 
  pp <- length(s)
  n <- length(dat$y)
  betavec <- s
  loglik <- 1e6
  res <- c(0, loglik, betavec)
  prevloglik <- Inf # To make sure it iterates 
  while (i < maxiter && abs(loglik - prevloglik) > tol && loglik < Inf) {
    i <- i + 1 
    prevloglik <- loglik
    for (j in 1:pp) {
      u <- dat$X %*% betavec
      expu <- exp(u) 
      prob <- expu/(expu+1)
      w <- prob*(1-prob) # weighted
      # avoid coeffcients diverging in order to achieve fitted  probabilities of 0 or 1.
      w <- ifelse(abs(w-0) < 1e-5, 1e-5, w)
      z <- u + (dat$y-prob)/w
      # calculate noj
      znoj <- dat$X[,-j] %*% betavec[-j]
      # revise the formula to be z
      betavec[j] <- sfun(mean(w*(dat$X[,j])*(z - znoj)), lambda)/(mean(w*dat$X[,j]*dat$X[,j]))
    }
    loglik <- sum(w*(z-dat$X %*% betavec)^2)/(2*n) + lambda * sum(abs(betavec))
    res <- rbind(res, c(i, loglik, betavec))}  
  return(res)
}
logmod <- cv.glmnet(newdat$X[,-dim(newdat$X)[2]], y=newdat$y, alpha=1, family="binomial",type.measure="mse")
```

**Logistic Regression:**

$y$: the vector of $n$ response random variable

$X$: the $n\times p$ design matrix ($X_{i}$ denote the $i$th row)

$\beta$: the $p\times 1$ coefficient

- Object: maximize log-likelihood function
$$
\begin{aligned}
max \sum_{i=1}^n \{y_i(X_{i}\beta)-\log(1+\exp(X_{i}\beta))\}
\end{aligned}
$$


### Newton Raphson


- The gradient:
$$
\nabla l(\beta) = X^T(y-p)
$$
where $p=\frac{\exp(X\beta)}{1+\exp(X\beta)}$

- The Hessian:
$$
\nabla^2 l(\beta) = -X^T W X
$$
where $W = diag(p_i(1-p_i)), i=1,\cdots,n$. The Hessian is negative definite.


### Newton Raphson

*Update coefficients: step-halving*
$$
\beta_{i+1}(\gamma) = \beta_{i} - \gamma[\nabla^2 l(\beta_{i})]^{-1}\nabla l(\beta_{i})
$$

- Set $\gamma = 1$
- If $f(\theta_{i+1}(1)) \geq f(\theta_{i})$, then set $\theta_{i+1} = \theta_{i+1}(1)$
- If $f(\theta_{i+1}(1)) \leq f(\theta_{i})$, search for a value $\gamma \in (0,1)$ for which $f(\theta_{i+1}(\gamma)) \geq f(\theta_{i})$, set $\theta_{i+1} = \theta_{i+1}(\gamma)$

*Gradient Descent:*

$$
\beta_{i+1} = \beta_{i} + H_{i}\nabla f(\beta_{i})
$$
where $H_{i} = (X^TX)^{-1}$ for every $i$.


### Logistic-LASSO Model with Pathwise Coordinate Descent

- Object: maximize the penalized log likelihood:
$$\max_{\beta \in \mathbb{R}^{p+1}} \frac{1}{n}\sum_{i=1}^n  \{y_i(X_{i}\beta)-\log(1+\exp(X_{i}\beta))\}-\lambda \sum_{j=0}^p |\beta_j|$$
for some $\lambda \geq 0$

- Coordinate-wise descent with weighted update:
$$
\tilde{\beta}^{lasso}_{j}(\lambda) \leftarrow \frac{S(\sum_{i=1}^{n}\omega_{i}x_{i,j}(y_{i} - \tilde{y_{i}}^{(-j)}), \lambda)}{\sum_{i=1}^n\omega_{i}x_{i,j}^{2}}
$$
where $\tilde{y_{i}}^{(-j)} = \sum_{k \neq j}x_{i,k}\tilde{\beta_{k}}$ and $S(\hat{\beta}, \lambda) = sign(\hat{\beta})(|\hat{\beta}| - \lambda)_{+}$



## Result
###  Estimation Path and Cross Validation for LASSO

```{r, echo=FALSE}
path <- function(inputx,inputy,grid){
  start <- rep(0, dim(inputx)[2])
  betas <- NULL
  for (x in 1:100) {
  cv.errors <- vector()
    cor.result <- coordinatelasso(lambda = grid[x], 
                                  dat = list(X=as.matrix(inputx),y=inputy),
                                  s= start)
    lasbeta <- cor.result[nrow(cor.result),3:dim(cor.result)[2]]
    start <- lasbeta
    betas <- rbind(betas,c(lasbeta))
  }
  return(data.frame(cbind(grid,betas)))
}
path.out <- read.csv("path.out.p2.csv",header = T)%>%
  dplyr::select(.,-c(1))
# plot a path of solutions
path.plot <- path.out %>%
  gather(key = par, value = estimate, c(2:dim(path.out)[2])) %>% 
  ggplot(aes(x = log(grid), y = estimate, group = par, col = par)) +
  geom_line()+
  ggtitle("Figure 2: A path of solutions with a sequence of descending lambda's") +
  xlab("log(Lambda)") + 
  ylab("Estimate") +
  theme(legend.position = "none", 
        legend.text = element_text(size = 6))
cvresult <- function(inputx,inputy,grid,K){
  n <- dim(inputx)[1]
  folds <- sample(1:K, n, replace=TRUE)
  start <- rep(0, dim(inputx)[2])
  cv.error <- vector()
  cv.se <- vector()
  for (x in 1:length(grid)) {
  cv.errors <- vector()
  for (i in 1:K){
    cor.result <- coordinatelasso(lambda = grid[x], 
                                  dat = list(X=as.matrix(inputx[folds!=i,]),y=inputy[folds!=i]),
                                  s = start)
    lasbeta <- cor.result[nrow(cor.result),3:dim(cor.result)[2]]
    u <- as.matrix(inputx[folds == i,]) %*% lasbeta
    expu <- exp(u) 
    prob <- expu / (1 + expu) 
    y <- as.vector(inputy[folds==i])
    cv.errors[i] = mean((y-prob)^2) 
    start <- lasbeta
  }
  cv.error[x] <- mean(cv.errors)
  cv.se[x] <- sqrt(var(cv.errors)/K)
  }
  return(cbind(grid,cv.error,cv.se))
}
result <- read.csv("cvresult.p2.csv",header = T)%>%
  dplyr::select(.,-c(1))
best.lambda <- result[which.min(result[,2]),1]
# need rewrite
finlasso <- as.matrix(path(newX,resp,grid=exp(seq(-8e-1,log(best.lambda), length=100))))
lasso.beta <- finlasso[nrow(finlasso),2:dim(finlasso)[2]]
# plot for cross validation
result <- data.frame(result)
cv.plot <- 
    ggplot(result, aes(x=log(result$grid), y=result$cv.error)) + 
    geom_errorbar(aes(ymin=result$cv.error-result$cv.se, ymax=result$cv.error+result$cv.se),
                  colour=1) +
    geom_line() +
    geom_point(size=0.8,colour = 4) +
    ggtitle("Figure 3: Lasso regression by 5 fold cross validation")+
    xlab("log(Lambda)") + ylab("MSE") +
    geom_vline(xintercept = log(best.lambda),col=3,lty=3) +
    annotate("text", log(best.lambda), 0.1, label = paste("best log(lambda) = ", round(log(best.lambda), 3), sep = ""))
grid.arrange(path.plot,cv.plot,ncol=2)
```


###  Model Comparison-Prediction Performance 

```{r, echo=FALSE}
######## compare prediction performance of all results
pred.fun <- function(outcome,input, beta){
    u <- as.matrix(input) %*% beta
    expu <- exp(u) 
    prob <- expu / (1 + expu) 
    pred.error = mean((as.vector(outcome)-prob)^2)
    return(pred.error)
}
# logistic regression by GLM
log.beta <- c(logit.beta[2:length(logit.beta)],logit.beta[1])
pred <- predict(logfit.1)
log.pred <- mean((resp-exp(pred)/(1+exp(pred)))^2) # abs(mean(logfit.1$residuals))
# newton's method
newton.ite <- nrow(newres)
newton.beta <- newres[nrow(newres),3:dim(newres)[2]]
newton.pred <- pred.fun(resp,newX,newton.beta)
# gradient decent
grad.ite <- nrow(gradres)
grad.beta <- gradres[nrow(gradres),3:dim(gradres)[2]]
grad.pred <- pred.fun(resp,newX,t(grad.beta))
logmod.cv <- cv.glmnet(newdat$X[,-dim(newdat$X)[2]], y=newdat$y, alpha=1, family="binomial")
best.lambda.func <- logmod$lambda.min
logmod <- glmnet(newdat$X[,-dim(newdat$X)[2]], y=newdat$y, alpha=1, family="binomial",lambda = best.lambda.func)
glmnet.beta <- c(coef.glmnet(logmod)[2:length(coef.glmnet(logmod))],coef.glmnet(logmod)[1])
glmnet.pred <- predict(logmod,as.matrix(newdataX),type="response", lambda = best.lambda.func)
glmnet.pred <- mean((resp-glmnet.pred)^2)
# lasso logistic
lasso.ite <- nrow(finlasso)
lasso.beta <- lasso.beta
lasso.pred <-  pred.fun(resp,newX,lasso.beta)

beta.res <- round(as.matrix(rbind(log.beta,newton.beta,grad.beta,lasso.beta,glmnet.beta)),2)
colnames(beta.res) <- colnames(newX)
rownames(beta.res) <- c("GLM package","Newton Raphson","Gradient Decent","Logistic Lasso","Lasso package")
perf.res <- matrix(rep(NA),ncol = 2, nrow = 5)
colnames(perf.res) <- c("iteration times","MSE")
rownames(perf.res) <- c("GLM package","Newton Raphson","Gradient Decent","Logistic Lasso","Lasso package")
perf.res[1,1] <- "NA"
perf.res[1,2] <- round(log.pred ,2)
perf.res[2,1] <- newton.ite
perf.res[2,2] <- round(newton.pred,2)
perf.res[3,1] <- grad.ite
perf.res[3,2] <- round(grad.pred,2)
perf.res[4,1] <- lasso.ite
perf.res[4,2] <- round(lasso.pred,2)
perf.res[5,1] <- "NA"
perf.res[5,2] <- round(glmnet.pred,2)

# output-performace
kable(t(perf.res), "latex", caption = "The comparison of performance for estimation algorithms and models", booktabs = T) %>%
kable_styling(latex_options = c("hold_position", "scale_down")) %>%
add_footnote(c("Dataset: Breast Cancer Diagnosis"),
notation = "alphabet")
```

### Model Comparison-Coefficient
```{r}
# output-beta
kable(t(beta.res), "latex", caption = "The comparison of performance for estimation algorithms and models", booktabs = T) %>%
kable_styling(latex_options = c("hold_position", "scale_down"), bootstrap_options = c("condensed")) %>%
add_footnote(c("Dataset: Breast Cancer Diagnosis"),
notation = "alphabet")
```


## Conclusion
### Conclusion and Discussion

- The results of our methods are compared to the the same parameter estimation as R's built-in packages
- Newton-Raphson has the convincing estimation and it converged quickly
- Gradient decent method showed similar estimation as Newton-Raphson method but it was less efficient
- For Pathwise Coordinate descent with LASSO logistic, according to the result of 5 fold cross validation and estimation result, the $\lambda$ with the lowest MSE and it shrunk six parameters to zero, which is comparable to the result by R's built-in packages. 

# Project: Bootstrap

## Background
### Down Syndrome Data

```{r, echo=F}
oridata <- read.csv("Down.csv")
mydata <- oridata%>%
  dplyr::select(.,-c(BAD_N,BCL2_N,H3AcK18_N,EGR1_N,H3MeK4_N))%>%
  filter(complete.cases(.))
```

The data Down.csv consists of the expression levels of 77 proteins/protein modifications that produced detectable signals in the nuclear fraction of cortex. It has 1080 rows and 79 columns. The first column MouseID identifies individual mice; The column 2-78 are values of expression levels of 77 proteins. Column 79 indicates whether the mouse is a control or has Down syndrome. The goal is to develop classification model based on the proteins expression levels.

### Missingness

- **Variable Selection: ** Delete variables with high missing rate $(\geq 15\%)$

Also due to the intrinsic correlation between individual proteins, it's impossible to apply normal regression methods to this dataset because of sigularity propblem. Instead, we choose regularized methods, LASSO, to be more specific.

### Missingness

```{r, dpi = 300, ehco=F}
x <- apply(is.na(oridata[,2:78]), 2, sum) / nrow(oridata)
tibble(name = names(x), percentage = as.numeric(x)) %>% 
  ggplot(aes(x = name, y = percentage)) + 
  geom_bar(stat = "identity") +
  labs(x = "proteins", y = "missing percentage") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 7))
### missing-------
n <- dim(mydata)[1]
ncol <- dim(mydata)[2]
list <- c(2:(ncol - 1))
namesX <- names(mydata)[-c(1,ncol)]
# standardize
dataX <- do.call(cbind, lapply(list, function(x) (mydata[,x] - mean(mydata[,x]))/sd(mydata[,x]))) 
colnames(dataX) <- namesX
# design matrix
X <- data.frame(dataX) %>% 
  mutate(., intercept = 1)
# response
y <- as.vector(ifelse(mydata[,ncol] == "Down syndrome", 1, 0))
dat <- list(y = y, X = as.matrix(X))
```

## Methods

### Pathwise Coordinate Descent with Regularized Logistic Regression

- Object: maximize the penalized log likelihood:
$$\max_{\beta \in \mathbb{R}^{p+1}} \frac{1}{n}\sum_{i=1}^n  \{y_i(X_{i}\beta)-\log(1+\exp(X_{i}\beta))\}-\lambda \sum_{j=0}^p |\beta_j|$$
for some $\lambda \geq 0$

- Coordinate-wise descent with weighted update:
$$
\tilde{\beta}^{lasso}_{j}(\lambda) \leftarrow \frac{S(\sum_{i=1}^{n}\omega_{i}x_{i,j}(y_{i} - \tilde{y_{i}}^{(-j)}), \lambda)}{\sum_{i=1}^n\omega_{i}x_{i,j}^{2}}
$$
where $\tilde{y_{i}}^{(-j)} = \sum_{k \neq j}x_{i,k}\tilde{\beta_{k}}$ and $S(\hat{\beta}, \lambda) = sign(\hat{\beta})(|\hat{\beta}| - \lambda)_{+}$


### Smoothed Bootstrap Estimation and Inference

- First we need to prepare a couple of candidate models 
- for each bootstrap in bootstrap with B times, select the best model and get estimates for the coefficient denoted as $t(y^*)$
- smooth $\hat{\mu} = t(y)$ by averaging over the bootstrap replications, defining
\[
\tilde{\mu} = s(y) = \frac{1}{B}\sum_{i=1}^B t(y^*)
\]

### Smoothed Bootstrap Estimation and Inference

And in addition to the percentile confidence interval, the nonparametric delta-method estimate of standard deviation for $s(y)$ in the nonideal case is:
\[
\tilde{sd}_B = [\sum_{i=1}^n\hat{cov}_j^2]^{1/2}
\]
where 
\[
\hat{cov}_j = \sum_{i=1}^B(Y_{ij}^*-Y_{.j}^*)(t_i^*-t_.^*)/B
\]
with $Y_{.j}^* = \sum_{i=1}^BY_{ij}^*/B$ and $t_.^* = \sum_{i=1}^Bt_{i}^*/B=s(y)$.

## Result

### Pathwise Coordinate Descent Logistic-LASSO

The left-hand side plot shows us that as the $\lambda$ increases, all the variable estimates of parameters shrink accordingly since we penalize all the parameters. When $\lambda = 0$, the result is the same as least square method and when $\lambda$ is too large, all the estimates of parameters shrink to 0. The right-hand side plot shows us the cross validation result for choosing the best $\lambda$. 

```{r, dpi = 300, echo=F, fig.height=4}
### regulized logistic regression: middle loop and inner loop
sfun <- function(beta,lambda) sign(beta) * max(abs(beta)-lambda, 0)
reglogitlasso <- function(lambda, dat, start, tol=1e-10, maxiter = 200,...){
  p <- dim(dat$X)[2]
  n <- length(dat$y)
  betavec <- start
  i <- 0 
  loglik <- 0
  prevloglik <- Inf 
  res <- c(0, loglik, betavec)
  while (i < maxiter && abs(loglik - prevloglik) > tol && loglik < Inf) {
    i <- i + 1 
    prevloglik <- loglik
    for (j in 1:p) {
      u <- dat$X %*% betavec
      expu <- exp(u) 
      prob <- expu/(expu+1)
      w <- prob*(1-prob) 
      w <- ifelse(abs(w-0) < 1e-5, 1e-5, w)
      z <- u + (dat$y-prob)/w
      znoj <- dat$X[,-j] %*% betavec[-j]
      betavec[j] <- sfun(mean(w*(dat$X[,j])*(z - znoj)), lambda)/(mean(w*dat$X[,j]*dat$X[,j]))
    }
    loglik <- sum(w*(z-dat$X %*% betavec)^2)/(2*n) + lambda * sum(abs(betavec))
    res <- rbind(res, c(i, loglik, betavec))}  
  return(res)
}
# intial = rep(0, dim(dat$X)[2])
# corres <- reglogitlasso(lambda = exp(-10), dat, start = intial,tol=1e-5) 
# start from -1 to -10

## pathwise update: outer loop
path <- function(data,grid){
  start <- rep(0, dim(data$X)[2])
  betas <- NULL
  for (x in 1:length(grid)){
    cor.result <- reglogitlasso(lambda = grid[x],dat = data,start= start)
    lasbeta <- cor.result[nrow(cor.result),3:dim(cor.result)[2]]
    start <- lasbeta
    betas <- rbind(betas,c(lasbeta))
  }
  return(data.frame(cbind(grid,betas)))
}
# path.out <- path(dat,grid=exp(seq(-1,-10, length=100)))
# colnames(path.out) <- c("grid",colnames(X))
path.out <- read.csv("path.out.csv",header = T)%>%
  dplyr::select(.,-c(1))
path.plot <- path.out %>%
  gather(key = par, value = estimate, c(2:dim(path.out)[2])) %>%
  ggplot(aes(x = log(grid), y = estimate, group = par, col = par)) +
  geom_line()+
  ggtitle("A) A path of solutions with a sequence of descending lambda's") +
  theme(plot.title = element_text(size = 8),legend.position = "none")+
  xlab("log(Lambda)") +
  ylab("Estimate") 
##### 5-fold cross-validation to choose beta lambda
cvresult <- function(dat,grid,K){
  n <- dim(dat$X)[1]
  folds <- sample(1:K, n, replace=TRUE)
  start <- rep(0, dim(dat$X)[2])
  cv.error <- vector()
  cv.se <- vector()
  for (x in 1:length(grid)) {
  cv.errors <- vector()
  for (i in 1:K){
    cor.result <- reglogitlasso(lambda = grid[x],dat = list(X=dat$X[folds!=i,],y=dat$y[folds!=i]),
                                start = start)
    lasbeta <- cor.result[nrow(cor.result),3:dim(cor.result)[2]]
    u <- as.matrix(dat$X[folds == i,]) %*% lasbeta
    expu <- exp(u) 
    prob <- expu / (1 + expu) 
    y <- as.vector(dat$y[folds==i])
    cv.errors[i] = mean((y-prob)^2) 
  }
  start <- lasbeta
  cv.error[x] <- mean(cv.errors)
  cv.se[x] <- sqrt(var(cv.errors)/K)
  }
  return(cbind(grid,cv.error,cv.se))
}
# result <- cvresult(dat,grid=exp(seq(-1,-10, length=100)),K=5) 
# best.lambda <- result[which.min(result[,2]),1]
best.lambda <- round(0.0004825801 ,6)
result <- read.csv("cvresult.csv",header = T)%>%
  dplyr::select(.,-c(1))
cv.plot <-
    ggplot(result, aes(x=log(result$grid), y=result$cv.error)) +
    geom_errorbar(aes(ymin=result$cv.error-result$cv.se, ymax=result$cv.error+result$cv.se),
                  colour=1) +
    geom_line() +
    geom_point(size=0.8,colour = 4) +
    ggtitle("B) LASSO regression by 5 fold cross validation")+
    theme(plot.title = element_text(size = 8))+
    xlab("log(Lambda)") + ylab("MSE") +
    geom_vline(xintercept = log(best.lambda),col=3,lty=3) +
    annotate("text", log(best.lambda)+2, 0.1, label = paste("best log(lambda) = ", round(log(best.lambda), 3), sep = ""))
grid.arrange(path.plot,cv.plot,ncol=2)
#### final estimation for beta
# finlasso <- as.matrix(path(dat,grid = exp(seq(-1,log(best.lambda), length=100))))
# colnames(finlasso) <- c("grid",colnames(X))
# lasso.beta <- finlasso[nrow(finlasso),2:dim(finlasso)[2]]
temp <- read.csv("coeff.csv",header = T)
lenb <- dim(temp)[1]
co.fin.beta <- rbind(temp[lenb,],temp[1:(lenb-1),])
# compare the result of our methods and glmnet
logmod <- glmnet(dat$X[,-dim(dat$X)[2]], y = dat$y, alpha=1, family="binomial",lambda = best.lambda)
res.comp <- cbind(co.fin.beta, as.matrix(coef.glmnet(logmod)))
names(res.comp) = c("protein", "LASSO", "glmnet") # it's very similar
```

### Model Selection Based on Smooth Bootstrap Estimation for Logistic-LASSO

algorithm:

- bootstrap data from the original dataset
- do cross validation and select the best $\lambda_{i}^{\ast}$ for each repetition
- calculate average $\lambda^{\ast} = \frac{1}{B}\sum_{i=1}^B \lambda_i^{\ast}$

We can see the discrepancy between results of PCD-LASSO and smooth bootstrap estimation for Logistic LASSO both in prediction and finding the best $\lambda$, the results of PCD-LASSO is deviated from the center of empirical distribution.

### Lambda Selection Based on Smooth Bootstrap Estimation for Logistic LASSO

```{r, dpi = 300, echo=F,fig.height=5}
############ get the estimate and se by smmothing bootstrap
n <- length(y)
p <- dim(dataX)[2]+1
B <- 5000
taskFun <- function(){
  # interest in: lambda,u_1(plot),beta,betanum, ynum
  ynum <- vector()
  betanum <- vector()
  beta <- vector()
  bootid <- sample(c(1:n),replace = T)
  boot.x <- dataX[bootid,]
  boot.y <- y[bootid]
  for (j in 1:n) ynum[j] <- sum(j==bootid)
  min.lambda <- cv.glmnet(x=boot.x, y=boot.y,alpha=1, family="binomial")$lambda.min
  model <- glmnet(x=boot.x, y=boot.y,alpha=1, family="binomial",lambda = min.lambda)
  beta <- coef(model)
  for (k in 1:p) betanum[k] <- I(coef(model)[k]!=0)
  sub1 <- predict(model,dataX)[1] #,type = "response"
  return(cbind(min.lambda,sub1,t(beta),t(betanum),t(ynum)))
}
# out <- foreach(i = 1:B, .combine = rbind) %dopar% taskFun()
# each row of output basicly tells you:
#   the best lambda
#   prediction of sub1
#   all the estimated beta
#   whether the covariate been chose or not
#   which obs been chose
# get sparse matrix, transform into data frame
out.df <- read.csv("bootstrap.csv",header = T)%>%
  dplyr::select(.,-c(1))
colnames(out.df) <- c("min.lambda","subject.1","intercpt",colnames(dataX),
                   sprintf("b[%d]",seq(1:p)),sprintf("n[%d]",seq(1:n)))
best.lambda.s <- mean(out.df$min.lambda)
# plot of expected value of subject 1
model <- glmnet(x=dataX, y=y,alpha=1, family="binomial",lambda = best.lambda)
sub1 <- predict(model,dataX)[1]#,type = "response"
s1 <- ggplot(out.df,aes(x=out.df$subject.1)) + 
  geom_histogram(colour="black", fill="white")+
  #geom_density()
  ggtitle("A) Expected value of subject 1 from smoothed bootstrap")+
  theme(plot.title = element_text(size = 8))+
  xlab("Expected value of subject 1") + ylab("Density") +
  geom_vline(xintercept = mean(out.df$subject.1),col=2,lty=2) + 
  geom_vline(xintercept = sub1,col=3,lty=3) 
s2 <- ggplot(out.df,aes(x=out.df$min.lambda)) +
  geom_histogram(colour="black", fill="white")+
  #geom_density()
  ggtitle("B) Penalty term from smoothed bootstrap")+
  theme(plot.title = element_text(size = 8))+
  xlab("Lambda") + ylab("Density") +
  geom_vline(xintercept = best.lambda.s,col=2,lty=2) + 
  geom_vline(xintercept = best.lambda,col=3,lty=3) 
grid.arrange(s1+
  annotation_custom(grid.text(paste("Expected value from data= ", round(sub1, 4), sep = ""),
                              x=0.3,y=0.5, hjust=0,
                   gp=gpar(col=3,fontsize=12, fontface="italic")))+
  annotation_custom(grid.text(paste("Expected value from bootstrap= ",
                                    round(mean(out.df$subject.1), 4), sep = ""),
                              x=0.3,y=0.6, hjust=0,
                   gp=gpar(col=2,fontsize=12, fontface="italic"))),
             s2+ 
  annotation_custom(grid.text(paste("Lambda from data= ", round(best.lambda, 6), sep = ""),
                              x=0.2,y=0.5, hjust=0,
                           gp=gpar(col=3,fontsize=12, fontface="italic")))+
  annotation_custom(grid.text( paste("Lambda from smoothed bootstrap = ", 
                                      round(best.lambda.s, 6),sep = ""),
                                x=0.1,y=0.6, hjust=0,
                           gp=gpar(col=2,fontsize=12, fontface="italic"))),
  ncol=2)
```


### Cross Validation for Model Prediction Comparison

We used 10 fold cross-validation to compare two different models, one is with $\lambda$ selected from data, the other is selected from the SBE. Table 1 shows us that while the Cross Validation MSE are similar between these two methods, smooth bootstrap estimation provides a more accurate classification result.

```{r, echo=FALSE}
################ cross validation for comparison of prediction error
cvcomp <- function(dat,K){
  n <- dim(dat$X)[1]
  folds <- sample(1:K, n, replace=TRUE)
  start <- rep(0, dim(dat$X)[2])
  cv.error.1 <- vector()
  cv.error.2 <- vector()
  cv.mse.1 <- vector()
  cv.mse.2 <- vector()
  for (i in 1:K){
    cv.x <- dat$X[folds!=i,]
    cv.y <- dat$y[folds!=i]
    obs <- glmnet(x=dat$X[folds!=i,],y=dat$y[folds!=i], alpha=1,family="binomial",lambda = best.lambda)
    smooth <- glmnet(x=dat$X[folds!=i,],y=dat$y[folds!=i], alpha=1,family="binomial",lambda = best.lambda.s)
    y.data <- predict(obs,dat$X[folds == i,],type = "response")>0.5
    y.smooth <- predict(smooth,dat$X[folds == i,],type = "response")>0.5
    y.data2 <- predict(obs,dat$X[folds == i,],type = "response")
    y.smooth2 <- predict(smooth,dat$X[folds == i,],type = "response")
    # misclassification rate
    cv.error.1[i] = sum(y.data!=dat$y[folds == i])/length(y[folds == i]) 
    cv.error.2[i] = sum(y.smooth!=dat$y[folds == i])/length(y[folds == i]) 
    # mse
    cv.mse.1[i] = mean((dat$y[folds == i]-y.data2)^2)
    cv.mse.2[i] = mean((dat$y[folds == i]-y.smooth2)^2)
  }
  return(list(error = cbind(mean(cv.error.1),mean(cv.error.2)),
              mse = cbind(mean(cv.mse.1),mean(cv.mse.2))))
}
cvresult <- rbind(round(cvcomp(dat,K=10)$error,4),round(cvcomp(dat,K=10)$mse,4))
colnames(cvresult) <- c("Penalty chosen by data","Penalty selected from smoothed bootstrap")
rownames(cvresult) <- c("Misclassification rate","Mean squred error")

kable(t(cvresult), "latex", caption = "The comparison of performance for two models", booktabs = T) %>%
kable_styling(latex_options = c("hold_position", "scale_down")) %>%
add_footnote(c("Dataset: Proteins expression levels of Down syndrome"),
notation = "alphabet")
```

### Significant Random Variable Selection from Smooth Bootstrap Estimation

Table 2 & 3 provide the full results of smooth bootstrap estimation for logistic LASSO. Our identification criterions here are: 

1. the chosen probability greater than 96\%

2. smooth bootstrap estimation confidence interval excludes zero. 

Based on that, we got 27 proteins that meets these two criterions (Table 4).

### Significant Proteins

```{r, echo=F}
## select beta based on number of chosen
# colnames(out.df) <- c("min.lambda","subject.1","intercpt",colnames(dataX),
#                    sprintf("b[%d]",seq(1:p)),sprintf("n[%d]",seq(1:n)))
betanum <- out.df[,c((2+p+1):(2+p+p))] # number of been chosen
betachosen <- round(t(apply(betanum,2,mean)),2)
## inference-hypothesis testing
betacoeff <- out.df[,c((2+1):(2+p))] # coefficients got from bootstrap, average it
ynum <- out.df[,c((2+2*p+1):(2+2*p+n))]
meany <- apply(ynum, 2, mean)
coef <- round(apply(betacoeff, 2, mean),4)
sd <- vector()
for (k in 1:p) {
  covj <- (t(ynum)-(as.vector(meany)%*%t(rep(1,B))))%*%(betacoeff[,k]-rep(coef[k],B))
  sd[k] <- round(sqrt((t(covj)%*%covj))/B,4)
}
coeff <- cbind(coef,sd) # mean and sd got from Efron methods
newcf <- round(cbind(lower.new = coef -1.96*sd,upper.new = coef +1.96*sd),4)
cf <- round(cbind(lower = apply(betacoeff,2,quantile,0.025),
                  upper = apply(betacoeff,2,quantile,0.975)),4) # CI got from empirical distritbuion
est.result <- cbind(origin =round(as.vector(co.fin.beta[,2]),4), # results from LASSO
                    prob = as.vector(betachosen),
                    coeff,cf,newcf)
rownames(est.result) <- c("Intercept", colnames(dataX))
# choose covariate based on this criterion
tmp = cbind(rownames(est.result), est.result)
covname = tmp %>% 
  as.tibble() %>% 
  filter(prob >= 0.96, 
         (lower.new > 0 | upper.new < 0)) %>% 
  select(V1) %>% 
  unlist()
kable(est.result[covname,], "latex", caption = "Significant Proteins", booktabs = T) %>%
  kable_styling(latex_options = c("hold_position", "scale_down"),
                font_size = 6,full_width = F) %>%
  add_footnote(c("Bootstap time=5000", "origin: estimation from PCD-LASSO", "prob: chosen probability from bootstrap", "doef: estimation from SBE", "sd: nonparamatric delta-method estimate of standard deviation", "lower, upper: quantile CI", "lower.new, upper.new: CI from nonparamatric delta-method estimate"),
               notation = "alphabet")
```

### Non-significant Proteins

```{r, echo=F}
kable(est.result[setdiff(rownames(est.result), covname),], "latex", caption = "Non-significant Proteins", booktabs = T) %>%
   kable_styling(latex_options = c("hold_position", "scale_down"),
                 font_size = 6, full_width = F, bootstrap_options = c("condensed")) %>%
   add_footnote(c("Bootstap time=5000", "origin: estimation from PCD-LASSO", "prob: chosen probability from bootstrap", "doef: estimation from SBE", "sd: nonparamatric delta-method estimate of standard deviation", "lower, upper: quantile CI", "lower.new, upper.new: CI from nonparamatric delta-method estimate"),
                notation = "alphabet")
```


## Conclusion

In this study, we first used cross validation to find the best $\lambda$ for PCD-LASSO prediction. Then we used bootstrap to plot empirical distribution of the best $\lambda$ for LASSO and the prediction of subject 1, which proved that classical statistical theory does ignore model selection in assessing estimation accuracy. The cross validation result of comparing PCD-LASSO best $\lambda$ and smooth bootstrap estimation best $\lambda$ also showed that smooth bootstrap estimation method provide a better result. Then based on the chosen probability and smooth bootstrap estimation CI, we identified a subset of proteins that are significantly associated with the Down syndrome.
